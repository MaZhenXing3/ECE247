{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.load(\"./project/X_test.npy\")\n",
    "y_test = np.load(\"./project/y_test.npy\")\n",
    "person_train_valid = np.load(\"./project/person_train_valid.npy\")\n",
    "X_train_valid = np.load(\"./project/X_train_valid.npy\")\n",
    "y_train_valid = np.load(\"./project/y_train_valid.npy\")\n",
    "person_test = np.load(\"./project/person_test.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y_train_valid)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y_test)\n",
    "y_train_valid -= 769\n",
    "y_test -= 769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.Y = torch.LongTensor(Y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#无downsample预处理过程\n",
    "# For subject 1\n",
    "X_train_valid_1 = X_train_valid[np.where(person_train_valid==0)[0]]\n",
    "y_train_valid_1 = y_train_valid[np.where(person_train_valid==0)[0]]\n",
    "X_test_1 = X_test[np.where(person_test==0)[0]]\n",
    "y_test_1 = y_test[np.where(person_test==0)[0]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_1, X_valid_1, y_train_1, y_valid_1 = train_test_split(X_train_valid_1, y_train_valid_1,\n",
    "                                                              test_size=0.2,shuffle=True,stratify=y_train_valid_1)\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "X_train_1 = X_train_1.transpose(0,2,1)\n",
    "X_valid_1 = X_valid_1.transpose(0,2,1)\n",
    "X_test_1 = X_test_1.transpose(0,2,1)\n",
    "train_set_1 = Dataset(X_train_1,y_train_1)\n",
    "val_set_1 = Dataset(X_valid_1,y_valid_1)\n",
    "test_set_1 = Dataset(X_test_1, y_test_1)\n",
    "train_loader_1 = torch.utils.data.DataLoader(train_set_1,batch_size=32,shuffle=True)\n",
    "val_loader_1 = torch.utils.data.DataLoader(val_set_1,batch_size=8,shuffle=True)\n",
    "test_loader_1 = torch.utils.data.DataLoader(test_set_1,batch_size=10,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_1.shape[0])\n",
    "for i, data in enumerate(train_loader_1):\n",
    "    print(data[0].shape, data[1].shape)\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#无downsample预处理过程\n",
    "# For all subjects\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid,\n",
    "                                                              test_size=0.2,shuffle=True)\n",
    "#--------------------------------------------------------------------------------------------------------\n",
    "X_train = X_train.transpose(0,2,1)\n",
    "X_valid = X_valid.transpose(0,2,1)\n",
    "X_test = X_test.transpose(0,2,1)\n",
    "train_set = Dataset(X_train,y_train)\n",
    "val_set = Dataset(X_valid,y_valid)\n",
    "test_set = Dataset(X_test, y_test)\n",
    "train_loader_1 = torch.utils.data.DataLoader(train_set,batch_size=288,shuffle=True)\n",
    "val_loader_1 = torch.utils.data.DataLoader(val_set,batch_size=72,shuffle=True)\n",
    "test_loader_1 = torch.utils.data.DataLoader(test_set,batch_size=90,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#包含嵘哥的downsample预处理过程\n",
    "# for one subject\n",
    "X_train_valid_1 = X_train_valid[np.where(person_train_valid==0)[0]]\n",
    "y_train_valid_1 = y_train_valid[np.where(person_train_valid==0)[0]]\n",
    "X_test_1 = X_test[np.where(person_test==0)[0]]\n",
    "y_test_1 = y_test[np.where(person_test==0)[0]]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid_1, y_train_valid_1,\n",
    "                                                              test_size=0.2,shuffle=True,stratify=y_train_valid_1)\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "num_time = X_train_valid.shape[2]\n",
    "sample_1 = list(np.arange(0,num_time,2))\n",
    "sample_2 = list(np.arange(1,num_time,2))\n",
    "\n",
    "X_train_1 = X_train[:,:,sample_1]\n",
    "X_train_2 = X_train[:,:,sample_2]\n",
    "\n",
    "X_val_1 = X_valid[:,:,sample_1]\n",
    "X_val_2 = X_valid[:,:,sample_2]\n",
    "\n",
    "X_test_s1 = X_test_1[:,:,sample_1]\n",
    "X_test_s2 = X_test_1[:,:,sample_2]\n",
    "\n",
    "X_train_s = np.concatenate((X_train_1,X_train_2), axis=0)\n",
    "y_train_s = np.concatenate((y_train,y_train), axis=0)\n",
    "\n",
    "X_val_s = np.concatenate((X_val_1,X_val_2), axis=0)\n",
    "y_val_s = np.concatenate((y_valid,y_valid), axis=0)\n",
    "\n",
    "#person_train_s = np.concatenate((person_train_valid,person_train_valid), axis=0)\n",
    "\n",
    "#X_test_s = np.concatenate((X_test_s1,X_test_s2), axis=0)\n",
    "X_test_s = X_test_s1\n",
    "#y_test_s = np.concatenate((y_test,y_test), axis=0)\n",
    "y_test_s = y_test_1\n",
    "#person_test_s = np.concatenate((person_test,person_test), axis=0)\n",
    "#person_test_s = person_test\n",
    "print(X_train_s.shape)\n",
    "print(y_train_s.shape)\n",
    "print(X_val_s.shape)\n",
    "print(X_test_s.shape)\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "X_train_s = X_train_s.transpose(0,2,1)\n",
    "X_val_s = X_val_s.transpose(0,2,1)\n",
    "X_test_s = X_test_s.transpose(0,2,1)\n",
    "train_set_1 = Dataset(X_train_s,y_train_s)\n",
    "val_set_1 = Dataset(X_val_s,y_val_s)\n",
    "test_set_1 = Dataset(X_test_s, y_test_s)\n",
    "train_loader_1 = torch.utils.data.DataLoader(train_set_1,batch_size=32,shuffle=True)\n",
    "val_loader_1 = torch.utils.data.DataLoader(val_set_1,batch_size=8,shuffle=True)\n",
    "test_loader_1 = torch.utils.data.DataLoader(test_set_1,batch_size=10,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_s.shape[0])\n",
    "for i, data in enumerate(train_loader_1):\n",
    "    print(data[0].shape, data[1].shape)\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 心爷版数据预处理\n",
    "# 把最后一个axis变两倍， 500 -> 1000\n",
    "def double_ax2(a):\n",
    "    result = np.zeros((a.shape[0],a.shape[1],a.shape[2] * 2))\n",
    "    b = np.reshape(a[:,:,-1],(a.shape[0],a.shape[1],1))\n",
    "    aMod = np.concatenate((a,b),axis = 2)\n",
    "    for i in range(a.shape[2]):\n",
    "        ave = (aMod[:,:,i] + aMod[:,:,i+1]) / 2\n",
    "        result[:,:,2 * i] = aMod[:,:,i]\n",
    "        result[:,:,2*i + 1] = ave\n",
    "    return result\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "X_train_valid_1 = X_train_valid[np.where(person_train_valid==0)[0]]\n",
    "y_train_valid_1 = y_train_valid[np.where(person_train_valid==0)[0]]\n",
    "\n",
    "\n",
    "X_test_1 = X_test[np.where(person_test==0)[0]]\n",
    "y_test_1 = y_test[np.where(person_test==0)[0]]\n",
    "#--------------------------------------------------------------------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_1, X_valid_1, y_train_1, y_valid_1 = train_test_split(X_train_valid_1, y_train_valid_1,\n",
    "                                                              test_size=0.2,shuffle=True,stratify=y_train_valid_1)\n",
    "\n",
    "\n",
    "print(X_train_1.shape)\n",
    "\n",
    "\n",
    "num_time = X_train_1.shape[2]\n",
    "sample_1 = list(np.arange(0,num_time,2))\n",
    "sample_2 = list(np.arange(1,num_time,2))\n",
    "\n",
    "X_tr_1 = X_train_1[:,:,sample_1]\n",
    "X_tr_2 = X_train_1[:,:,sample_2]\n",
    "#X_test_s1 = X_test[:,:,sample_1]\n",
    "#X_test_s2 = X_test[:,:,sample_2]\n",
    "print('X_tr_1,  ',X_tr_1.shape)\n",
    "\n",
    "#X_train_s = X_tr_1   # 这个只用了两组中的一组\n",
    "X_train_s = np.concatenate((X_tr_1,X_tr_2), axis=0)  # 这个是两组都加进去， N 变成 N*3\n",
    "#y_train_s = y_train_1  # 这个只用了两组中的一组\n",
    "y_train_s = np.concatenate((y_train_1,y_train_1), axis=0) # 这个是两组都加进去， N 变成 N*3\n",
    "print('X_train_s  ',X_train_s.shape)\n",
    "X_train_s_x2 = double_ax2(X_train_s)\n",
    "print(X_train_s_x2.shape)\n",
    "X_train_s = np.concatenate((X_train_1,X_train_s_x2), axis=0)\n",
    "print(X_train_s.shape)\n",
    "\n",
    "y_train_s = np.concatenate((y_train_1,y_train_s), axis=0)\n",
    "print(y_train_s.shape)\n",
    "\n",
    "#person_train_valid_s = np.concatenate((person_train_valid,person_train_valid,person_train_valid), axis=0)\n",
    "#person_test_s = person_test\n",
    "#X_test_s = np.concatenate((X_test_s1,X_test_s2), axis=0)\n",
    "X_test_s = X_test_1\n",
    "#X_test_s1\n",
    "#y_test_s = np.concatenate((y_test,y_test), axis=0)\n",
    "y_test_s = y_test_1\n",
    "#y_test\n",
    "#person_test_s = np.concatenate((person_test,person_test), axis=0)\n",
    "\n",
    "print(X_train_s.shape)\n",
    "print(y_train_s.shape)\n",
    "print(X_test_s.shape)\n",
    "#print(person_train_s.shape)\n",
    "#print(person_test.shape)\n",
    "X_train_s = X_train_s.transpose(0,2,1)\n",
    "X_valid_1 = X_valid_1.transpose(0,2,1)\n",
    "X_test_1 = X_test_1.transpose(0,2,1)\n",
    "\n",
    "train_set_1 = Dataset(X_train_s,y_train_s)\n",
    "val_set_1 = Dataset(X_valid_1,y_valid_1)\n",
    "test_set_1 = Dataset(X_test_1, y_test_1)\n",
    "\n",
    "train_loader_1 = torch.utils.data.DataLoader(train_set_1,batch_size=96,shuffle=True)\n",
    "val_loader_1 = torch.utils.data.DataLoader(val_set_1,batch_size=8,shuffle=True)\n",
    "test_loader_1 = torch.utils.data.DataLoader(test_set_1,batch_size=5,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_s.shape[0])\n",
    "for i, data in enumerate(test_loader_1):\n",
    "    print(data[0].shape, data[1].shape)\n",
    "    if i == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#和CNN.ipynb中一样的结构，这里想训练出一个较好的CNN并且提取前两层作为之后的LSTM之前用来提取特征的工具\n",
    "# [conv-relu]*2 -> 2*2 max-pooling -> [conv-relu]*3 -> 2*2 max_pooling -> (affine-relu)*2 -> affine -> softmax\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()  # initial the model\n",
    "        self.conv1 = nn.Conv1d(22,40,kernel_size = 2,stride = 2) \n",
    "        self.bn1 = nn.BatchNorm1d(40)\n",
    "        self.conv2 = nn.Conv1d(40,60,kernel_size = 3,stride = 1) \n",
    "        self.bn2 = nn.BatchNorm1d(60) \n",
    "        self.pool1 = nn.MaxPool1d(2,2) \n",
    "        \n",
    "        self.conv3 = nn.Conv1d(60,80,kernel_size = 3, stride = 1) \n",
    "        self.bn3 = nn.BatchNorm1d(80)\n",
    "        self.conv4 = nn.Conv1d(80,100,kernel_size = 3, stride = 1) \n",
    "        self.bn4 = nn.BatchNorm1d(100)\n",
    "        self.conv5 = nn.Conv1d(100,120,kernel_size = 3, stride = 2) #120*122\n",
    "        self.bn5 = nn.BatchNorm1d(120)\n",
    "        self.pool2 = nn.MaxPool1d(2,2) #120*61\n",
    "        \n",
    "        self.fc1 = nn.Linear(120*61, 300) # input dim , output dim\n",
    "        self.bn6 = nn.BatchNorm1d(300)\n",
    "        self.drop1 = nn.Dropout(0.8)\n",
    "        self.fc2 = nn.Linear(300,40)  \n",
    "        self.bn7 = nn.BatchNorm1d(40)\n",
    "        self.drop2 = nn.Dropout(0.8)\n",
    "        self.fc3 = nn.Linear(40,4)\n",
    " \n",
    "    def forward(self,x):\n",
    "        x = torch.Tensor(x.numpy().transpose(0,2,1))\n",
    "        x = self.pool1(F.elu(self.bn2(self.conv2(F.elu(self.bn1(self.conv1(x)))))))\n",
    "        x = F.elu(self.bn3(self.conv3(x)))\n",
    "        x = F.elu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool2(F.elu(self.bn5(self.conv5(x))))\n",
    "        x = x.view(-1,120*61)\n",
    "        \n",
    "        x = self.drop1(F.elu(self.bn6(self.fc1(x))))\n",
    "        x = self.drop2(F.elu(self.bn7(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(net.parameters(),lr = 0.01)\n",
    "optimizer = torch.optim.RMSprop(net.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(net.parameters(),lr = 0.001)\n",
    "optimizer = torch.optim.RMSprop(net.parameters(),lr = 0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9, nesterov = True, weight_decay = 5e-4)\n",
    "val_acc_history = []\n",
    "train_acc_history = []\n",
    "for epoch in range(30):\n",
    "    for i , data in enumerate(train_loader_1, 0):\n",
    "        inputs, labels = data\n",
    "        \n",
    "        net.train()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        train_acc = (predicted == labels).sum().item() / len(labels)\n",
    "        \n",
    "        if i % 2 == 1:\n",
    "            net.eval()\n",
    "            val_correct, val_total = 0, 0 \n",
    "            for val_data in val_loader_1:\n",
    "                val_images, val_labels = val_data\n",
    "                val_outputs = net(val_images)\n",
    "                _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "                val_total += val_labels.size(0)\n",
    "                val_correct += (val_predicted == val_labels).sum().item()\n",
    "            val_acc = val_correct / val_total\n",
    "            print('Epoch {} | Iter{} | Loss{:.4f} | TrainAcc{:.4f} | val acc {:.4f}'.format(\n",
    "                epoch+1, i , loss, train_acc, val_acc))\n",
    "            #writer.add_scalar('Train/Loss',loss,epoch*len(trainloader) + i)\n",
    "            #writer.add_scalar('Train/ACC',train_acc,epoch*len(trainloader) + i)\n",
    "            #writer.add_scalar('VAL/ACC',val_acc,epoch*len(trainloader) + i)\n",
    "    net.eval()\n",
    "    train_correct, train_total = 0, 0\n",
    "    for train_data in train_loader_1:\n",
    "        train_inputs, train_labels = train_data\n",
    "        train_outputs = net(train_inputs)\n",
    "        _, train_predicted = torch.max(train_outputs.data, 1)\n",
    "        train_total += train_labels.size(0)\n",
    "        train_correct += (train_predicted == train_labels).sum().item()\n",
    "    train_acc = train_correct / train_total\n",
    "    train_acc_history.append(train_acc)\n",
    "    net.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    for val_data in val_loader_1:\n",
    "        val_images, val_labels = val_data\n",
    "        val_outputs = net(val_images)\n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "        val_total += val_labels.size(0)\n",
    "        val_correct += (val_predicted == val_labels).sum().item()\n",
    "    val_acc = val_correct / val_total\n",
    "    val_acc_history.append(val_acc)\n",
    "    #if val_acc == max(val_acc_history):\n",
    "    #    net_best = Net()\n",
    "    #    net_best.load_state_dict(net.state_dict())\n",
    "net.eval() \n",
    "test_correct, test_total = 0, 0\n",
    "for test_data in test_loader_1:\n",
    "    test_inputs, test_labels = test_data\n",
    "    \n",
    "    test_outputs = net(test_inputs)\n",
    "    #test_outputs = net_best(test_images)\n",
    "    _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "    test_total += test_labels.size(0)\n",
    "    test_correct += (test_predicted == test_labels).sum().item()\n",
    "test_acc = test_correct / test_total\n",
    "print('Test accuracy is: ',test_acc)\n",
    "plt.plot(train_acc_history,)\n",
    "plt.plot(val_acc_history)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend(['train_acc','val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个只包含上面的CNN的前两层的class\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet,self).__init__()  # initial the model\n",
    "        self.conv1 = nn.Conv1d(22,40,kernel_size = 2,stride = 2) \n",
    "        self.bn1 = nn.BatchNorm1d(40)\n",
    "        self.conv2 = nn.Conv1d(40,60,kernel_size = 3,stride = 1) \n",
    "        self.bn2 = nn.BatchNorm1d(60) \n",
    "        self.pool1 = nn.MaxPool1d(2,2) \n",
    " \n",
    "    def forward(self,x):\n",
    "        x = torch.Tensor(x.numpy().transpose(0,2,1))\n",
    "        x = self.pool1(F.elu(self.bn2(self.conv2(F.elu(self.bn1(self.conv1(x)))))))\n",
    "        x = torch.Tensor(x.detach().numpy().transpose(0,2,1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把已经train了的CNN的前两层提取出来，作为之后的LSTM之前用来提取特征的工具\n",
    "convnet = ConvNet()\n",
    "pretrained_dict = net.state_dict()\n",
    "convnet_dict = convnet.state_dict()\n",
    "pretrained_dict = {k:v for k,v in pretrained_dict.items() if k in convnet_dict}\n",
    "convnet_dict.update(pretrained_dict)\n",
    "convnet.load_state_dict(convnet_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两层CNN + 三层LSTM\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.conv = convnet  #这个是用已经训练过得到的CNN\n",
    "        #self.conv = ConvNet()  #这个是用没有训练过的CNN\n",
    "        self.LSTM1 = nn.LSTM(input_dim, hidden_dim1, bidirectional = True, num_layers=1, batch_first = True, dropout = 0)\n",
    "        self.LSTM2 = nn.LSTM(hidden_dim1*2, hidden_dim2, bidirectional = True, num_layers=2, batch_first = True, dropout = 0.4)\n",
    "        self.LSTM3 = nn.LSTM(hidden_dim2*2, hidden_dim3, bidirectional = True, num_layers=2, batch_first = True, dropout = 0.4)\n",
    "        #self.LSTM4 = nn.LSTM(hidden_dim3*2, hidden_dim4, bidirectional = True, num_layers=1, batch_first = True, dropout = 0)\n",
    "        #self.fc1 = nn.Linear(hidden_dim3*2*249, 2000) #要取决于用那种数据，249对应1000的数据，124对应嵘哥的downsample500的数据（嵘哥ds！）\n",
    "        self.fc1 = nn.Linear(hidden_dim3*2*249, 2000) \n",
    "        self.bn1 = nn.BatchNorm1d(2000)\n",
    "        self.drop1 = nn.Dropout(0.7)\n",
    "        self.fc2 = nn.Linear(2000,100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.drop2 = nn.Dropout(0.7)\n",
    "        self.fc3 = nn.Linear(100,20)\n",
    "        self.bn3 = nn.BatchNorm1d(20)\n",
    "        self.drop3 = nn.Dropout(0.8)\n",
    "        self.fc4 = nn.Linear(20,output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, h=None, c=None):\n",
    "        x = self.conv(x)\n",
    "        if type(h) == type(None) and type(c) == type(None):\n",
    "            out, (hn, cn) = self.LSTM1(x)\n",
    "            out, (hn, cn) = self.LSTM2(out)\n",
    "            out, (hn, cn) = self.LSTM3(out)\n",
    "            #out, (hn, cn) = self.LSTM4(out)\n",
    "        else:\n",
    "            out, (hn, cn) = self.LSTM1(x, h.detach(), c.detach())\n",
    "            out, (hn, cn) = self.LSTM2(out, h.detach(), c.detach())\n",
    "            out, (hn, cn) = self.LSTM3(out, h.detach(), c.detach())\n",
    "            #out, (hn, cn) = self.LSTM4(out, h.detach(), c.detach())\n",
    "        #out = self.drop1(F.elu(self.bn1(self.fc1(out[:, -1, :]))))\n",
    "        out = self.drop1(F.elu(self.bn1(self.fc1(out.reshape(out.shape[0],-1)))))\n",
    "        out = self.drop2(F.elu(self.bn2(self.fc2(out))))\n",
    "        out = self.drop3(F.elu(self.bn3(self.fc3(out))))\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 60\n",
    "hidden_dim1 = 40\n",
    "hidden_dim2 = 60\n",
    "hidden_dim3 = 80\n",
    "output_dim = 4\n",
    "model = LSTM(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(),alpha = 0.99, lr=0.001, weight_decay = 0.01)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay = 0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9, nesterov = True, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_history = []\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "test_loss_history = []\n",
    "t0 = time.time()\n",
    "num_epochs = 125\n",
    "for epoch in range(num_epochs):\n",
    "    tstart = time.time()\n",
    "    for i, data in enumerate(train_loader_1):\n",
    "        inputs, labels = data\n",
    "        #inputs = convnet(inputs)\n",
    "        #inputs = torch.Tensor(inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        train_acc = (predicted == labels).sum().item() / len(labels)\n",
    "        \n",
    "    model.eval()\n",
    "    train_correct, train_total = 0, 0\n",
    "    train_loss = 0\n",
    "    for train_data in train_loader_1:\n",
    "        train_inputs, train_labels = train_data\n",
    "        #train_inputs = convnet(train_inputs)\n",
    "        #train_inputs = torch.Tensor(train_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "        train_outputs = model(train_inputs)\n",
    "        _, train_predicted = torch.max(train_outputs.data, 1)\n",
    "        train_total += train_labels.size(0)\n",
    "        train_correct += (train_predicted == train_labels).sum().item()\n",
    "        train_loss += criterion(train_outputs, train_labels).item()\n",
    "    train_acc = train_correct / train_total\n",
    "    train_acc_history.append(train_acc)\n",
    "    train_loss_history.append(train_loss)\n",
    "        \n",
    "    #pXtrain = model(Xtrain)\n",
    "    #ptrain = torch.argmax(pXtrain, axis = 1)\n",
    "    #train_acc = np.mean(ptrain.numpy() == ytrain.numpy())\n",
    "    #train_accs.append(train_acc)\n",
    "    #tloss = criterion(pXtrain, ytrain)\n",
    "    #train_losses.append(tloss.item())\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    val_loss = 0\n",
    "    for val_data in val_loader_1:\n",
    "        val_inputs, val_labels = val_data\n",
    "        #val_inputs = convnet(val_inputs)\n",
    "        #val_inputs = torch.Tensor(val_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "        val_outputs = model(val_inputs)\n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "        val_total += val_labels.size(0)\n",
    "        val_correct += (val_predicted == val_labels).sum().item()\n",
    "        val_loss += criterion(val_outputs, val_labels).item()\n",
    "    val_acc = val_correct / val_total\n",
    "    val_acc_history.append(val_acc)\n",
    "    val_loss_history.append(val_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    test_loss = 0\n",
    "    for test_data in test_loader_1:\n",
    "        test_inputs, test_labels = test_data\n",
    "        #test_inputs = convnet(test_inputs)\n",
    "        #test_inputs = torch.Tensor(test_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "        test_outputs = model(test_inputs)\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "        test_total += test_labels.size(0)\n",
    "        test_correct += (test_predicted == test_labels).sum().item()\n",
    "        test_loss += criterion(test_outputs, test_labels).item()\n",
    "    test_acc = test_correct / test_total\n",
    "    test_acc_history.append(test_acc)\n",
    "    test_loss_history.append(test_loss)\n",
    "    \n",
    "    #pXval = model(Xval)\n",
    "    #pval = torch.argmax(pXval, axis = 1)\n",
    "    #val_acc = np.mean(pval.numpy() == yval.numpy())\n",
    "    #val_accs.append(val_acc)\n",
    "    #vloss = criterion(pXval, yval)\n",
    "    #val_losses.append(vloss.item())\n",
    "    tend = time.time()\n",
    "    print('epoch: {:<3d}    time: {:<3.2f}    train_loss: {:<3.3f}    train acc: {:<1.3f}    val_loss: {:<3.3f}    val acc: {:<1.3f}   test_loss: {:<3.3f}    test acc: {:<1.3f}'.format(epoch+1, \n",
    "            tend - tstart, train_loss, train_acc, val_loss, val_acc, test_loss, test_acc))\n",
    "time_total = time.time() - t0\n",
    "print('Total time: {:4.3f} seconds, average time per epoch: {:4.3f}'.format(time_total, time_total / num_epochs))\n",
    "model.eval()\n",
    "test_correct, test_total = 0, 0\n",
    "for test_data in test_loader_1:\n",
    "    test_inputs, test_labels = test_data\n",
    "    #test_inputs = convnet(test_inputs)\n",
    "    #test_inputs = torch.Tensor(test_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "    test_outputs = model(test_inputs)\n",
    "    _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "    test_total += test_labels.size(0)\n",
    "    test_correct += (test_predicted == test_labels).sum().item()\n",
    "test_acc = test_correct / test_total\n",
    "print('Test accuracy is: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_history)\n",
    "plt.plot(val_acc_history)\n",
    "plt.title('training and validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_history)\n",
    "plt.plot(val_loss_history)\n",
    "plt.title('loss history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "print('Test accuracy is: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两层CNN + 三层GRU\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
    "        super(GRU, self).__init__()\n",
    "        self.conv = convnet  #这个是用已经训练过得到的CNN\n",
    "        #self.conv = ConvNet()  #这个是用没有训练过的CNN\n",
    "        self.GRU1 = nn.GRU(input_dim, hidden_dim1, bidirectional = True, num_layers=1, batch_first = True, dropout = 0)\n",
    "        self.GRU2 = nn.GRU(hidden_dim1*2, hidden_dim2, bidirectional = True, num_layers=2, batch_first = True, dropout = 0.4)\n",
    "        self.GRU3 = nn.GRU(hidden_dim2*2, hidden_dim3, bidirectional = True, num_layers=2, batch_first = True, dropout = 0.4)\n",
    "        self.fc1 = nn.Linear(hidden_dim3*2*249, 2000) #要取决于用那种数据，249对应未处理的数据，124对应嵘哥的downsample数据（嵘哥ds！）\n",
    "        self.bn1 = nn.BatchNorm1d(2000)\n",
    "        self.drop1 = nn.Dropout(0.8)\n",
    "        self.fc2 = nn.Linear(2000,100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.drop2 = nn.Dropout(0.8)\n",
    "        self.fc3 = nn.Linear(100,20)\n",
    "        self.bn3 = nn.BatchNorm1d(20)\n",
    "        self.drop3 = nn.Dropout(0.8)\n",
    "        self.fc4 = nn.Linear(20,output_dim)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        x = self.conv(x)\n",
    "        if type(h) == type(None):\n",
    "            out, hn = self.GRU1(x)\n",
    "            out, hn = self.GRU2(out)\n",
    "            out, hn = self.GRU3(out)\n",
    "        else:\n",
    "            out, hn = self.GRU1(x, h.detach())\n",
    "            out, hn = self.GRU2(out, h.detach())\n",
    "            out, hn = self.GRU3(out, h.detach())\n",
    "        #out = self.drop1(F.relu(self.bn1(self.fc1(out[:, -1, :]))))\n",
    "        out = self.drop1(F.elu(self.bn1(self.fc1(out.reshape(out.shape[0],-1)))))\n",
    "        out = self.drop2(F.elu(self.bn2(self.fc2(out))))\n",
    "        out = self.drop3(F.elu(self.bn3(self.fc3(out))))\n",
    "        out = self.fc4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 60\n",
    "hidden_dim1 = 40\n",
    "hidden_dim2 = 60\n",
    "hidden_dim3 = 80\n",
    "output_dim = 4\n",
    "model = GRU(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(),alpha = 0.99, lr=0.001, weight_decay = 0.01)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001,weight_decay = 0.01)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum = 0.9, nesterov = True, weight_decay = 5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc_history = []\n",
    "train_acc_history = []\n",
    "test_acc_history = []\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "test_loss_history = []\n",
    "t0 = time.time()\n",
    "num_epochs = 125\n",
    "for epoch in range(num_epochs):\n",
    "    tstart = time.time()\n",
    "    for i, data in enumerate(train_loader_1):\n",
    "        inputs, labels = data\n",
    "        #inputs = convnet(inputs)\n",
    "        #inputs = torch.Tensor(inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _,predicted = torch.max(outputs.data, 1)\n",
    "        train_acc = (predicted == labels).sum().item() / len(labels)\n",
    "        \n",
    "    model.eval()\n",
    "    train_correct, train_total = 0, 0\n",
    "    train_loss = 0\n",
    "    for train_data in train_loader_1:\n",
    "        train_inputs, train_labels = train_data\n",
    "        #train_inputs = convnet(train_inputs)\n",
    "        #train_inputs = torch.Tensor(train_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "        train_outputs = model(train_inputs)\n",
    "        _, train_predicted = torch.max(train_outputs.data, 1)\n",
    "        train_total += train_labels.size(0)\n",
    "        train_correct += (train_predicted == train_labels).sum().item()\n",
    "        train_loss += criterion(train_outputs, train_labels).item()\n",
    "    train_acc = train_correct / train_total\n",
    "    train_acc_history.append(train_acc)\n",
    "    train_loss_history.append(train_loss)\n",
    "        \n",
    "    #pXtrain = model(Xtrain)\n",
    "    #ptrain = torch.argmax(pXtrain, axis = 1)\n",
    "    #train_acc = np.mean(ptrain.numpy() == ytrain.numpy())\n",
    "    #train_accs.append(train_acc)\n",
    "    #tloss = criterion(pXtrain, ytrain)\n",
    "    #train_losses.append(tloss.item())\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    val_loss = 0\n",
    "    for val_data in val_loader_1:\n",
    "        val_inputs, val_labels = val_data\n",
    "        #val_inputs = convnet(val_inputs)\n",
    "        #val_inputs = torch.Tensor(val_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "        val_outputs = model(val_inputs)\n",
    "        _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "        val_total += val_labels.size(0)\n",
    "        val_correct += (val_predicted == val_labels).sum().item()\n",
    "        val_loss += criterion(val_outputs, val_labels).item()\n",
    "    val_acc = val_correct / val_total\n",
    "    val_acc_history.append(val_acc)\n",
    "    val_loss_history.append(val_loss)\n",
    "    \n",
    "    model.eval()\n",
    "    test_correct, test_total = 0, 0\n",
    "    test_loss = 0\n",
    "    for test_data in test_loader_1:\n",
    "        test_inputs, test_labels = test_data\n",
    "        #test_inputs = convnet(test_inputs)\n",
    "        #test_inputs = torch.Tensor(test_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "        test_outputs = model(test_inputs)\n",
    "        _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "        test_total += test_labels.size(0)\n",
    "        test_correct += (test_predicted == test_labels).sum().item()\n",
    "        test_loss += criterion(test_outputs, test_labels).item()\n",
    "    test_acc = test_correct / test_total\n",
    "    test_acc_history.append(test_acc)\n",
    "    test_loss_history.append(test_loss)\n",
    "    \n",
    "    #pXval = model(Xval)\n",
    "    #pval = torch.argmax(pXval, axis = 1)\n",
    "    #val_acc = np.mean(pval.numpy() == yval.numpy())\n",
    "    #val_accs.append(val_acc)\n",
    "    #vloss = criterion(pXval, yval)\n",
    "    #val_losses.append(vloss.item())\n",
    "    tend = time.time()\n",
    "    print('epoch: {:<3d}    time: {:<3.2f}    train_loss: {:<3.3f}    train acc: {:<1.3f}    val_loss: {:<3.3f}    val acc: {:<1.3f}   test_loss: {:<3.3f}    test acc: {:<1.3f}'.format(epoch+1, \n",
    "            tend - tstart, train_loss, train_acc, val_loss, val_acc, test_loss, test_acc))\n",
    "time_total = time.time() - t0\n",
    "print('Total time: {:4.3f} seconds, average time per epoch: {:4.3f}'.format(time_total, time_total / num_epochs))\n",
    "model.eval()\n",
    "test_correct, test_total = 0, 0\n",
    "for test_data in test_loader_1:\n",
    "    test_inputs, test_labels = test_data\n",
    "    #test_inputs = convnet(test_inputs)\n",
    "    #test_inputs = torch.Tensor(test_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "    test_outputs = model(test_inputs)\n",
    "    _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "    test_total += test_labels.size(0)\n",
    "    test_correct += (test_predicted == test_labels).sum().item()\n",
    "test_acc = test_correct / test_total\n",
    "print('Test accuracy is: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_acc_history)\n",
    "plt.plot(val_acc_history)\n",
    "plt.title('training and validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_loss_history)\n",
    "plt.plot(val_loss_history)\n",
    "plt.title('loss history')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train', 'val'])\n",
    "plt.show()\n",
    "\n",
    "test_correct, test_total = 0, 0\n",
    "for test_data in test_loader_1:\n",
    "    test_inputs, test_labels = test_data\n",
    "    #test_inputs = convnet(test_inputs)\n",
    "    #test_inputs = torch.Tensor(test_inputs.detach().numpy().transpose(0,2,1)) #249*60\n",
    "    test_outputs = model(test_inputs)\n",
    "    _, test_predicted = torch.max(test_outputs.data, 1)\n",
    "    test_total += test_labels.size(0)\n",
    "    test_correct += (test_predicted == test_labels).sum().item()\n",
    "test_acc = test_correct / test_total\n",
    "print('Test accuracy is: ',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
